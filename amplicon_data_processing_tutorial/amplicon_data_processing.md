#Data Processing in the Fierer Lab#**Updated: 2/19/2015**  **Questions: leff.jonathan@gmail.com**##Logging inWe will be working with our lab server for this tutorial. Its name is microbe. Your user name is probably your last name plus your first initial (mine is "leffj", for example). To use microbe, open a terminal window, and type and hit return:	ssh <your microbe user name>@microbe.colorado.edu##For your first time###Setting up your passwordWhen you log in for the first time, you will have to set a new password. First, log in using your temporary password. When you set a new password, make sure that it is something secure (i.e. has at least letters and numbers in it). Note, nothing will show up on the screen when you enter passwords.Important: Please be respectful and do not give your PW out to other people. The server is currently accessible to the whole world, so if your PW falls into the wrong hands, this will make a lot more work for me.##Processing Data###Step 0: make a directory to store your filesLet's stay organized:	mkdir tutorial	cd tutorial	pwd###Step 1: get your mapping file in orderDownload the demo mapping file using this link: [http://fiererlab.org/?p=516](http://fiererlab.org/?p=516)To transfer files to the server, use the command line:	scp <mapping file path> <microbe user name>@microbe.colorado.edu:<path where you want it to go/>	If you have a PC, you can simply copy the mapping file from another server location using this command. In the future, you will have to learn another method to transfer files:	cp /data/shared/2014_02_03_data_tutorial/Demo_16S_MappingFile.txt .Let's check the mapping file to make sure it is formatted correctly (QIIME command):	validate_mapping_file.py -h	validate_mapping_file.py -m Demo_16S_MappingFile.txt -o checkout/	less checkout/Demo_16S_MappingFile.log	rm -r checkout	(**** Be extra careful with the rm command!!!!*****#$%#%#)###Step 2: prepare the sequence data**Important** All Fierer lab raw sequence data should be stored in `/data/shared/`For example, to see the data for this tutorial:	ls /data/shared/2014_02_03_data_tutorial/Note the naming convention for directories in `/data/shared`: YYYY\_MM\_DD\_NameTo start processing these data, use this command. This step is called demultiplexing.	prep_fastq_for_uparse_paired.py -h	prep_fastq_for_uparse_paired.py -i /data/shared/2014_02_03_data_tutorial/Undetermined_S0_L001_R1_001_t.fastq.gz -r /data/shared/2014_02_03_data_tutorial/Undetermined_S0_L001_R2_001_t.fastq.gz -b /data/shared/2014_02_03_data_tutorial/Undetermined_S0_L001_I1_001_t.fastq.gz -m Demo_16S_MappingFile.txt -o demultiplexed_seqs/ -cMerge paired end reads using this command. Note that settings are just guesses and can be altered:	usearch7 -fastq_mergepairs demultiplexed_seqs/demultiplexed_seqs_1.fq -reverse demultiplexed_seqs/demultiplexed_seqs_2.fq -fastqout demultiplexed_seqs/demultiplexed_seqs_merged.fq -fastq_truncqual 3 -fastq_maxdiffs 1 -fastq_minovlen 10 -fastq_minmergelen 200  ###Step 3: Prepare sequences for de novo database creation![Pipeline_diagram](pipeline_diag.jpg)####Check quality of sequences (optional)	usearch7 -fastq_stats demultiplexed_seqs/demultiplexed_seqs_merged.fq -log demultiplexed_seqs/demultiplexed_seqs_merged.log	less demultiplexed_seqs/demultiplexed_seqs_merged.log####Conduct quality filtering	usearch7 -fastq_filter demultiplexed_seqs/demultiplexed_seqs_merged.fq -fastaout seqs_filt.fa -fastq_maxee 0.5####Dereplicate sequences	usearch7 -derep_fulllength seqs_filt.fa -output seqs_filt_derep.fa -sizeout####Remove singleton sequences	usearch7 -sortbysize seqs_filt_derep.fa -output seqs_filt_derep2.fa -minsize 2###Step 4: Cluster filtered sequences to create de novo databaseThis is done at the 97% similarity level by default	usearch7 -cluster_otus seqs_filt_derep2.fa -otus rep_set.faNeed to renumber the sequences to have convenient OTU names:	fasta_number.py rep_set.fa "OTU_" > rep_set_numbered.faAt this stage the de novo database can be optionally filtered against an existing public database to remove highly divergent sequences###Step 5: Convert raw sequences to fasta format (from fastq)This can be run while step 4 is still running. This is simply a formatting step.	usearch7 -fastq_filter demultiplexed_seqs/demultiplexed_seqs_merged.fq -fastaout demultiplexed_seqs/demultiplexed_seqs_merged.fa###Step 6: Map the raw/demultiplexed (fasta formatted) sequences to the de novo database and build the OTU table	usearch7 -usearch_global demultiplexed_seqs/demultiplexed_seqs_merged.fa -db rep_set_numbered.fa -id 0.97 -strand plus -uc readmap.uc	create_otu_table_from_uc_file.py -i readmap.uc -o otu_table.txt###Step 7: Add taxonomic classificationsThe goal of this step is to provide taxonomic classifications for each OTU. We do this using the RDP classifier with the GreenGenes database.	biom convert -i otu_table.txt -o otu_table.biom --table-type 'OTU table' --to-json	assign_taxonomy.py -m rdp -i rep_set_numbered.fa -o rdp_assigned_taxonomy -c 0.5 -t /db_files/gg_files/gg_13_8_otus/taxonomy/97_otu_taxonomy.txt -r /db_files/gg_files/gg_13_8_otus/rep_set/97_otus.fasta --rdp_max_memory 10000	biom add-metadata -i otu_table.biom --observation-metadata-fp rdp_assigned_taxonomy/rep_set_numbered_tax_assignments.txt --sc-separated taxonomy --observation-header OTUID,taxonomy -o otu_table_wTax.biom###Step 8: Rarefy and summarize the OTU table by taxonomyIt is a good idea to check for and remove chloroplast and mitochondria sequences in most sample types:	filter_taxa_from_otu_table.py -i otu_table_wTax.biom -o otu_table_wTax_noChloroMito.biom -n c__Chloroplast,f__mitochondriaCheck the number of sequences per sample:	biom summarize-table -i otu_table_wTax_noChloroMito.biom -o otu_table_wTax_noChloroMito_smry.txtRarefy:	single_rarefaction.py -i otu_table_wTax_noChloroMito.biom -o otu_table_wTax_noChloroMito_500.biom -d 500###Step 9: Odds and endsConvert rarefied OTU table back to text format:	biom convert -i otu_table_wTax_noChloroMito_500.biom -o otu_table_wTax_noChloroMito_500.txt --to-tsv --header-key=taxonomySummarize the taxonomic output:	summarize_taxa.py -i otu_table_wTax_noChloroMito_500.biom -o sumtax --suppress_biom_table_outputMove files to local directory (in terminal on local computer):	scp <user name>@microbe.colorado.edu:/data/<user name>/tutorial/ otu_table_wTax_noChloroMito_500.biom .	scp -r <user name>@microbe.colorado.edu:/data/<user name>/tutorial/sumtax .##Extra Credit \-\- Avoid entering microbe PW every timeYou can configure ssh, so that as long as you have one terminal window connected to microbe, you won't have to enter your password when you connect via other windows. For instance, if you are browsing files on microbe using ssh and then want to transfer them to your laptop, keep the ssh window open and use scp from your laptop in another terminal window, and you won't need to enter your password. This saves some time and annoyance.To do the configuration:	vi ~/.ssh/configuse the text editor (`i` for insert mode, remember) to add the following text:	Host microbe		HostName microbe.colorado.edu		ControlMaster auto		ControlPath ~/.ssh/sockets/%r@%h:%p	User <microbe user name>Press `esc`, then `:wq`Now, whenever you would've used `microbe.colorado.edu`, you can just type, `microbe`